{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<ul> <li> <p>Rust Axum:</p> <ul> <li> <p>Axum is so power full !</p> </li> <li> <p>FromRef to extract sub-state</p> </li> </ul> </li> <li> <p>Rust basic:</p> <ul> <li> <p>Brieft introduction to Rust</p> </li> <li> <p>Async Await are just the key words for Concurrency</p> </li> <li> <p>Memory layout of some data types</p> </li> <li> <p>Trait bound and ways for dispatching</p> </li> <li> <p>How Closure is implemented</p> </li> <li> <p>Enum is not just an integer</p> </li> <li> <p>Send &amp; Sync trait</p> </li> <li> <p>#[Derive Clone]</p> </li> </ul> </li> <li> <p>Linux kernel</p> <ul> <li> <p>Crash recovery and debugging</p> </li> <li> <p>Watchdog module</p> </li> <li> <p>Preempt</p> </li> <li> <p>Minirootfs Alpine &amp; internet connection</p> </li> </ul> </li> <li> <p>Network</p> <ul> <li> <p>Tunnel interface</p> </li> <li> <p>QEMU Cluster</p> </li> </ul> </li> <li> <p>Others</p> <ul> <li> <p>Conditional Variable and how channel in Rust is implemented</p> </li> <li> <p>Dependency Injection</p> </li> </ul> </li> </ul> Todo!  * Rust Database:      * [Database](rust/database/database.md)      * [sqlx](rust/database/database.md)      * [Sea-querry, an ORM](rust/database/seaquerry.md)      * [Modern SQL](rust/database/modql.md)"},{"location":"tags/","title":"Tags","text":"<p>{% for tag in tags %} - {{ tag.name }} ({{ tag.count }})</p>"},{"location":"kernel/crash_recovery/crash_recovery/","title":"Crash recovery and debugging","text":"<p>This artical introduces a way that Linux Kernel can quickly recover from a crash and also support capturing the contents of system memory for further debugging</p>"},{"location":"kernel/crash_recovery/crash_recovery/#the-big-picture","title":"The big picture","text":"<p>[Product Kernel] --crash--&gt; [Crash Kernel] --&gt; [Save crash dump] --&gt; [Reboot/Analyze]</p> <p>In the very first boot, reserve a small memory in RAM by using boot option, e.g: <code>crashkernel=200M</code></p> <p>When product kernel goes live, using <code>kexec</code> to load a smaller kernel + initrd, let's call it capture-kernel, on to the reserve region above.</p> <p>In case our system (with product kernel) suffers a crash, the capture-kernel is automatically booted up.</p> <p>Notice that the capture kernel should only live inside the resevered memory if we want to do further debugging.</p> <p>Using kdump captures the whole system memory to a dumpfile. The dumpfile then can be stored somewhere, on persistent storage, for further debugging.</p>"},{"location":"kernel/crash_recovery/crash_recovery/#kexec","title":"Kexec","text":"<ul> <li> <p>kexec allows you to load and boot a new kernel directly from a running system without going through the BIOS/firmware or reboot cycle.</p> </li> <li> <p>This is especially useful for:</p> <ul> <li>Faster reboots (skip hardware init)</li> <li>Crash recovery (boot into a crash kernel to capture memory dumps)</li> </ul> </li> </ul>"},{"location":"kernel/crash_recovery/crash_recovery/#kdump","title":"Kdump","text":"<ul> <li>Dumps the main kernel's memory into <code>/var/crash/vmcore</code> using <code>makedumpfile</code> or <code>kdumpctl</code></li> </ul>"},{"location":"kernel/crash_recovery/crash_recovery/#crash","title":"Crash","text":"<ul> <li> <p>Inspect the <code>vmcore</code>:</p> <pre><code>crash /usr/lib/debug/vmlinux-$(uname -r) /var/crash/vmcore\n</code></pre> </li> </ul>"},{"location":"kernel/crash_recovery/crash_recovery/#practice-on-alpine","title":"Practice on Alpine","text":""},{"location":"kernel/crash_recovery/crash_recovery/#create-custom-image","title":"create custom image","text":"<ul> <li> <p>Create ram based image which contains kexec</p> <ul> <li>Ref: Minirootfs Alpine &amp; internet connection</li> </ul> </li> </ul> <pre><code># fetch minirootfs\nwget https://dl-cdn.alpinelinux.org/alpine/v3.22/releases/x86_64/alpine-minirootfs-3.22.0-x86_64.tar.gz\n\n# extracting\nmkdir rootfs\ntar -xzf alpine-minirootfs-3.22.0-x86_64.tar.gz -C rootfs\n\n# bind with host for further chroot usage\ncd rootfs\nmount --bind /proc proc\nmount --bind /dev dev\nmount --bind /sys sys\n\n# add DNS\necho \"nameserver 8.8.8.8\" &gt; etc/resolv.conf\n\n# change root\nchroot . /bin/sh\n\n# add feature: \napk add alpine-base linux-virt mkinitfs\n\n# keep terminal in chroot for further setup\n</code></pre>"},{"location":"kernel/crash_recovery/crash_recovery/#install-kexec","title":"install kexec","text":"<pre><code># add repo. notice the http (not https)\necho \"http://dl-cdn.alpinelinux.org/alpine/v3.22/main\" &gt;&gt; /etc/apk/repositories\necho \"http://dl-cdn.alpinelinux.org/alpine/v3.22/community\" &gt;&gt; /etc/apk/repositories\napk update\n\n# install kexec\napk add kexec-tools\n</code></pre>"},{"location":"kernel/crash_recovery/crash_recovery/#prepare-capture-image","title":"prepare capture image","text":"<ul> <li> <p>Store whatever capture kernel+initrd as you want.</p> </li> <li> <p>Here, I use the same product <code>vmlinuz-virt</code> + <code>initramfs-virt</code>, located at <code>boot</code>, as capture image</p> </li> </ul> <pre><code>/ # cd boot/\n/boot # tree\n.\n\u251c\u2500\u2500 System.map-6.12.51-0-virt\n\u251c\u2500\u2500 boot -&gt; .\n\u251c\u2500\u2500 config-6.12.51-0-virt\n\u251c\u2500\u2500 initramfs-virt\n\u2514\u2500\u2500 vmlinuz-virt\n</code></pre>"},{"location":"kernel/crash_recovery/crash_recovery/#prepare-mkinitfs-features","title":"prepare mkinitfs features","text":"<pre><code># add kexec to mkinitfs\ncat &gt; /etc/mkinitfs/features.d/kexec.files &lt;&lt;'EOF'\n/usr/sbin/kexec\nEOF\n\n# add capture_kernel\ncat &gt; /etc/mkinitfs/features.d/capture_kernel.files &lt;&lt;'EOF'\n/boot/\nEOF\n\n## some tweaks\n# add dns\necho \"/etc/resolv.conf\" &gt;&gt; /etc/mkinitfs/features.d/base.files\n# add apk update\necho \"/etc/apk/world\" &gt;&gt; /etc/mkinitfs/features.d/base.files\necho \"/var/lib/\" &gt;&gt; /etc/mkinitfs/features.d/base.files\necho \"/lib/apk/db\" &gt;&gt; /etc/mkinitfs/features.d/base.files\n##\n</code></pre>"},{"location":"kernel/crash_recovery/crash_recovery/#make-initfs","title":"make initfs","text":"<pre><code>mkinitfs -F \"base virtio kexec capture_kernel\" -k 6.12.51-0-virt\n# ==&gt; initramfs: creating /boot/initramfs-virt for 6.12.51-0-virt\n</code></pre>"},{"location":"kernel/crash_recovery/crash_recovery/#boot-up-the-new-image","title":"boot up the new image","text":"<ul> <li> <p>Add boot option <code>crashkernel</code> to Qemu command</p> </li> <li> <p>Notice that we need to enable kexec using boot option <code>kexec_load_disabled</code> as Alpine disable this feature by default.</p> </li> </ul> <pre><code># Qemu\nsudo qemu-system-x86_64 \\\n-m 512 \\\n-kernel vmlinuz-virt \\\n-initrd initramfs-virt \\\n-append \"console=ttyS0 init=/init crashkernel=128M kexec_load_disabled=0\" \\\n-nographic\n\n# check inside Qemu node\n~ # uname -r\n6.12.51-0-virt\n~ # ls /boot/\nSystem.map-6.12.51-0-virt  initramfs-virt\nconfig-6.12.51-0-virt      vmlinuz-virt\n~ # which kexec\n/usr/sbin/kexec\n</code></pre> <ul> <li>Note: in case facing apk error, try to create dir <code>/var/lib</code></li> </ul> <pre><code>~ # apk update\nERROR: Failed to open apk database: No such file or directory\n\n~ # mkdir /var/lib -p\n~ # apk update\n\nOK: 49 distinct packages available\n~ # \n</code></pre>"},{"location":"kernel/crash_recovery/crash_recovery/#manualy-switch-to-another-kernel","title":"manualy switch to another kernel","text":"<ul> <li> <p>Use option <code>kexec -l</code> to load capture image</p> </li> <li> <p>Use <code>kexec -e</code> to trigger the switching</p> </li> </ul> <pre><code># use \nkexec -l /boot/vmlinuz-virt \\\n  --initrd=/boot/initramfs-virt \\\n  --command-line=\"init=/init console=ttyS0\"\n</code></pre> <ul> <li>Note: the <code>/proc/iomem</code> has changed after switching</li> </ul> Memory Map 1 (Old) Memory Map 2 (New) <code>00100000-1ffdffff : System RAM</code> <code>00100000-1ffdffff : System RAM</code> <code>0b000000-0bdfffff : Kernel code</code> <code>1d000000-1ddfffff : Kernel code</code> <code>0be00000-0c703fff : Kernel rodata</code> <code>1de00000-1e703fff : Kernel rodata</code> <code>0c800000-0c9f247f : Kernel data</code> <code>1e800000-1e9f247f : Kernel data</code> <code>0d0bb000-0d1fffff : Kernel bss</code> <code>1f0bb000-1f1fffff : Kernel bss</code> <code>16000000-1dffffff : Crash kernel</code> (none)"},{"location":"kernel/crash_recovery/crash_recovery/#automatically-switch-when-crashing","title":"Automatically switch when crashing","text":"<ul> <li>Use <code>kexec -p</code> as <code>p</code> stands for panic.</li> </ul> <pre><code>kexec -p /boot/vmlinuz-virt \\\n  --initrd=/boot/initramfs-virt \\\n  --command-line=\"init=/init console=ttyS0 maxcpus=1\"\n</code></pre> <ul> <li>By default, alpine disable <code>PROC_KCORE</code>, it means we must re-build the alpine's kernel with this option enabled. So, I decided to end this artical here :(</li> </ul> <pre><code>/boot # cat config-6.12.51-0-virt  | grep -i kcore\n# CONFIG_PROC_KCORE is not set\n</code></pre>"},{"location":"kernel/linker_cache/linker_cache/","title":"Dynamic linker cache","text":""},{"location":"kernel/mini_alpine/mini_alpine/","title":"Mini alpine","text":"<ul> <li> <p>Goal 1: Create &amp; boot a pair of vmlinuz+initrd from alpine-minirootfs</p> <ul> <li> <p>Small size image <pre><code>-rw-------  1 root root 5.6M Jun  8 21:24 initramfs-virt\n-rw-r--r--  1 root root  12M Jun  8 07:09 vmlinuz-virt\n</code></pre></p> </li> <li> <p>Run in RAM only: <pre><code>qemu-system-x86_64 \\\n-m 512M \\\n-kernel vmlinuz-virt \\\n-initrd initramfs-virt \\\n-netdev tap,id=net0,ifname=tap0,script=no,downscript=no \\\n-device virtio-net-pci,netdev=net0 \\\n-append \"console=ttyS0 init=/init\" \\\n-enable-kvm \\\n-nographic\n</code></pre></p> </li> </ul> </li> <li> <p>Goad 2: provide internet connection via NAT</p> <p></p> </li> </ul>"},{"location":"kernel/mini_alpine/mini_alpine/#goal-1","title":"Goal 1","text":""},{"location":"kernel/mini_alpine/mini_alpine/#steps","title":"Steps:","text":"<pre><code>#\nwget https://dl-cdn.alpinelinux.org/alpine/v3.22/releases/x86_64/alpine-minirootfs-3.22.0-x86_64.tar.gz\n#\nmkdir rootfs\ntar -xzf alpine-minirootfs-3.22.0-x86_64.tar.gz -C rootfs\n#\ncd rootfs\n#\nmount --bind /proc proc\nmount --bind /dev dev\nmount --bind /sys sys\n#\necho \"nameserver 8.8.8.8\" &gt; etc/resolv.conf\n#\nchroot . /bin/sh\n#\napk add alpine-base linux-virt mkinitfs\n#   /lib/modules/6.12.31-0-virt\nmkinitfs -k 6.12.31-0-virt\n\n\n# images located at boot\nls /boot\n</code></pre>"},{"location":"kernel/mini_alpine/mini_alpine/#detail-each-steps","title":"Detail each steps:","text":"<ul> <li> <p>Download minirootfs from alpine homepage</p> <pre><code>#\nwget https://dl-cdn.alpinelinux.org/alpine/v3.22/releases/x86_64/alpine-minirootfs-3.22.0-x86_64.tar.gz\n#\nmkdir rootfs\n#\ntar -xzf alpine-minirootfs-3.22.0-x86_64.tar.gz -C rootfs\n</code></pre> </li> <li> <p>chroot to rootfs</p> <pre><code># bind host -&gt; rootfs\ncd rootfs\nmount --bind /proc proc\nmount --bind /dev dev\nmount --bind /sys sys\n\n# provide DNS for rootfs\necho \"nameserver 8.8.8.8\" &gt; etc/resolv.conf\n\n# chroot to rootfs\nchroot . /bin/sh\n</code></pre> </li> <li> <p>inside chroot, update package</p> <pre><code>apk add alpine-base linux-virt mkinitfs\n</code></pre> <ul> <li> <p>Note1. Change repository by:     <pre><code>echo \"http://dl-cdn.alpinelinux.org/alpine/v3.22/main\" &gt;&gt; /etc/apk/repositories\n</code></pre></p> </li> <li> <p>Note 2. <code>alpine-base</code> provide:     busybox \u2013 provides standard UNIX tools (sh, ls, mount, ifconfig, etc.)</p> <p>alpine-baselayout \u2013 sets up filesystem hierarchy (/etc, /var, /run, etc.)</p> <p>alpine-conf \u2013 contains init scripts, system config (/etc/inittab, setup-* scripts)</p> <p>libcrypt \u2013 password hashing</p> <p>libgcc \u2013 runtime library for GCC-built programs</p> <p>musl \u2013 the C standard library</p> <p>openrc \u2013 the default init system in Alpine</p> <p>apk-tools \u2013 the Alpine package manager (apk)</p> </li> <li> <p>Note3. <code>linux-virt</code> is suitable with QEMU. </p> <ul> <li>This package provide:<ul> <li>/boot/vmlinuz-virt</li> <li>/lib/module/6.12.31-0-virt</li> </ul> </li> <li><code>linux-lts</code> for normal hardware</li> </ul> </li> <li> <p>Note 4. Install some usefull package basedon your need:</p> <pre><code>apk add iproute2\n</code></pre> </li> </ul> </li> <li> <p>Make initrd</p> <pre><code> mkinitfs -F \"base virtio\" -k 6.12.31-0-virt\n==&gt; initramfs: creating /boot/initramfs-virt for 6.12.31-0-virt\n</code></pre> <ul> <li> <p>Node 0. <code>base virtio</code> is features</p> <ul> <li>by default, mkinitfs builds features inside <code>/etc/mkinitfs/mkinitfs.conf</code>,</li> <li>but, it not work in my env, so I specify it in the call to <code>mkinitfs</code></li> </ul> </li> <li> <p>Note 1. <code>/etc/mkinitfs/mkinitfs.conf</code> contain features for initrd</p> <pre><code># default configure is:\n#    features=\"ata base cdrom ext4 keymap kms mmc nvme raid scsi usb virtio\"\n#\n\n#\n# custom mkinitfs conf:\n#\n# Specify features to include\nfeatures=\"base,virtio\"\n\n# Kernel modules path (optional, defaults to /lib/modules/&lt;version&gt;)\nmodloop=/lib/modules/6.12.31-0-virt\n\n# Additional files (optional, if not using feature.d/*.files)\nadd_files=\"/sbin/openrc-init /bin/sh\"\n</code></pre> </li> <li> <p>Note 2. files &amp; modules are defined at <code>/etc/mkinitfs/features.d</code></p> <ul> <li>E.g: define new features     <pre><code>/etc/mkinitfs/features.d # cat netfilter.modules \nkernel/net/netfilter/*\n\n/etc/mkinitfs/features.d # cat netfilter.files\n/usr/sbin/ipvsadm\n</code></pre></li> </ul> </li> </ul> </li> <li> <p>QEMU boot</p> <pre><code>qemu-system-x86_64 \\\n-m 512 \\\n-kernel vmlinuz-virt \\\n-initrd initramfs-virt \\\n-append \"console=ttyS0 init=/init\" \\\n-nographic\n</code></pre> </li> </ul>"},{"location":"kernel/mini_alpine/mini_alpine/#goal-2","title":"Goal 2","text":"<ul> <li> <p>Create bridge <code>br0</code> and <code>tap0</code></p> <pre><code>sudo ip link add name br0 type bridge\nsudo ip addr add 192.168.100.1/24 dev br0\nsudo ip link set br0 up\n\n# create tap device\nsudo ip tuntap add dev tap0 mode tap user $USER\nsudo ip link set tap0 master br0\nsudo ip link set tap0 up\n</code></pre> </li> <li> <p>route between <code>eth0</code> -&gt; <code>br0</code>:</p> <pre><code># alow traffic from br0 -&gt; eth0 -&gt; internet\niptables -A FORWARD -i br0 -o eth0 -j ACCEPT\n\n# alow traffic back from eth0 -&gt; br0\niptables -A FORWARD -i eth0 -o br0 -m state --state ESTABLISHED,RELATED -j ACCEPT\n\n# NAT\n sudo iptables -t nat -A POSTROUTING -s 192.168.100.0/24 -o eth0 -j MASQUERADE\n</code></pre> </li> <li> <p>boot QEMU with</p> <pre><code>qemu-system-x86_64 \\\n-m 512M \\\n-kernel vmlinuz-virt \\\n-initrd initramfs-virt \\\n-netdev tap,id=net0,ifname=tap0,script=no,downscript=no \\\n-device virtio-net-pci,netdev=net0 \\\n-append \"console=ttyS0 init=/init\" \\\n-enable-kvm \\\n-nographic\n</code></pre> <pre><code># config ip\nip a add 192.168.100.200/24 dev eth0\nip link set eth0 up\n\n# config route\nip route add default via 192.168.100.1 dev eth0\n\n# DNS\necho \"nameserver 8.8.8.8\" &gt; /etc/resolv.conf\n</code></pre> <ul> <li> <p>Note 0: use <code>virtio-net-pci</code> device because Alpine has virtio driver     <pre><code>lsmod | grep vir\nvirtio_net            118784  0 \nnet_failover           24576  1 virtio_net\n</code></pre></p> </li> <li> <p>Note 1:     <pre><code># route\nip route\ndefault via 192.168.100.1 dev eth0 \n192.168.100.0/24 dev eth0 scope link  src 192.168.100.200 \n\n# ip addr\nip a show eth0\n2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000\n    link/ether 52:54:00:12:34:56 brd ff:ff:ff:ff:ff:ff\n    inet 192.168.100.200/24 scope global eth0\n    valid_lft forever preferred_lft forever\n    inet6 fe80::5054:ff:fe12:3456/64 scope link \n    valid_lft forever preferred_lft forever\n\n# ping local\nping -c 3 192.168.100.1\nPING 192.168.100.1 (192.168.100.1): 56 data bytes\n64 bytes from 192.168.100.1: seq=0 ttl=64 time=0.203 ms\n64 bytes from 192.168.100.1: seq=1 ttl=64 time=0.198 ms\n64 bytes from 192.168.100.1: seq=2 ttl=64 time=0.220 ms\n\n--- 192.168.100.1 ping statistics ---\n3 packets transmitted, 3 packets received, 0% packet loss\nround-trip min/avg/max = 0.198/0.207/0.220 ms\n\n# ping google\nping -c 3 google.com\nPING google.com (142.250.198.206): 56 data bytes\n64 bytes from 142.250.198.206: seq=0 ttl=114 time=51.240 ms\n64 bytes from 142.250.198.206: seq=1 ttl=114 time=51.227 ms\n64 bytes from 142.250.198.206: seq=2 ttl=114 time=51.228 ms\n\n--- google.com ping statistics ---\n3 packets transmitted, 3 packets received, 0% packet loss\nround-trip min/avg/max = 51.227/51.231/51.240 ms\n</code></pre></p> </li> </ul> </li> </ul>"},{"location":"kernel/mini_alpine/mini_alpine/#active-apk","title":"Active APK","text":"<ul> <li>First call to <code>apk</code> was failed</li> </ul> <pre><code>$ apk update\nERROR: Unable to lock database: No such file or directory\nERROR: Failed to open apk database: No such file or directory\n</code></pre> <ul> <li> <p>The reason is mini-rootfs does not have 3 files/direct: <code>/etc/apk/world</code> and <code>/var/lib/</code> and <code>/lib/apk/db</code></p> <ul> <li>Just create the 2 files/dir <pre><code>#\ntouch /etc/apk/world\n# must add `alpine-base` \necho \"alpine-base\" &gt; /etc/apk/world\necho \"openssl\" &gt;&gt; /etc/apk/world\n\n#\nmkdir -p /var/lib\nmkdir -p /lib/apk/db\n\n# this will work\napk update\n</code></pre></li> </ul> </li> <li> <p>Note: </p> <ul> <li> <p>the lock database is just a file located in <code>/lib/apk/db/lock</code></p> </li> <li> <p>Edit repository Url: <pre><code>echo \"http://dl-cdn.alpinelinux.org/alpine/v3.22/main\" &gt; /etc/apk/repositories\n</code></pre></p> </li> <li> <p>Add openrc: <pre><code>apk fix openrc\n</code></pre></p> </li> </ul> </li> </ul>"},{"location":"kernel/mini_alpine/mini_alpine/#active-openssh","title":"Active Openssh","text":"<pre><code># install\napk add openssh\n\n# create dummy user and keygen\necho 'sshd:x:50:50:sshd:/var/empty:/sbin/nologin' &gt;&gt; /etc/passwd\n/usr/bin/ssh-keygen -A\n\n# Start\n/usr/sbin/sshd -D -e &amp;\n</code></pre> <pre><code># restart sshd\n# Stop if running\npkill sshd\n\n# Start fresh\n/usr/sbin/sshd -D -e &amp;\n</code></pre>"},{"location":"kernel/preempt/preempt/","title":"Preempt","text":"<ul> <li> <p>There are 3 Preempt mode</p> <ul> <li> <p>NONE : no preempt</p> </li> <li> <p>Voluntary : task voluntarily gives up CPU to other task</p> </li> <li> <p>Full : task is forced to give up CPU</p> </li> </ul> </li> <li> <p>There are 2 approachs to config preempt mode</p> <ul> <li> <p>Static:</p> <ul> <li>Set mode in COMPILE time, and can not change after that</li> </ul> </li> <li> <p>Dynamic:</p> <ul> <li>CONFIG_PREEMPT_DYNAMIC = y</li> <li>Chose a default mode in COMPILE time</li> <li>Can change mode on runtime via boot params</li> </ul> </li> </ul> </li> </ul> <p></p>"},{"location":"kernel/watchdog/watchdog/","title":"Watchdog","text":"<ul> <li> <p>Watchdog is a Hardware device arms to reset the machine if the machine does not send PING to watchdog after a period of time.</p> </li> <li> <p>Watchdog holds a Counter Register</p> <ul> <li> <p>Counter initialy is set to a value (e.g: 600 sec)</p> </li> <li> <p>Counter is decreased over time</p> </li> <li> <p>If Counter = 0, Watchdog trigger machine to reboot</p> </li> <li> <p>When machine send PING to Watchdog, Counter is reloaded (set back to 600 sec)</p> </li> </ul> <p></p> </li> <li> <p>When watchdog kernel module is loaded, it will check if Watchdog device is running or not?</p> <ul> <li>E.g: iTCO_wdt_probe</li> </ul> <pre><code>static int iTCO_wdt_probe(struct platform_device *pdev) {\n    ...\n\n    if (!iTCO_wdt_set_running(p)) {\n        /*\n        * If the watchdog was not running set NO_REBOOT now to\n        * prevent later reboots.\n        */\n        p-&gt;update_no_reboot_bit(p-&gt;no_reboot_priv, true);\n    }\n\n    ...\n}\n</code></pre> </li> <li> <p>Use PING from Kernel via: CONFIG_WATCHDOG_HANDLE_BOOT_ENABLED</p> <ul> <li> <p>Some Linux distro will force to has Watchdog running after boot.</p> </li> <li> <p>So, it requires PING to keep the COUNTER away from zero</p> </li> <li> <p>PING can come from process in UserSpace</p> </li> <li> <p>OR, PING from KERNEL via CONFIG_WATCHDOG_HANDLE_BOOT_ENABLED:</p> <ul> <li> <p>The config get kernel to send ping to Watchdog after boot.</p> </li> <li> <p>Kernel will stop send PING when UserSpace starts to send it's first PING.</p> </li> </ul> </li> </ul> </li> </ul>"},{"location":"networking/docker_network/images/docker_network/","title":"Docker Networking","text":""},{"location":"networking/load_balancer/ipvs_load_balancer/","title":"IPVS Load balancer","text":""},{"location":"networking/qemu_network/qemu/","title":"QEMU cluster","text":"<ul> <li>Goal 1: Create and connect multiple Qemu nodes</li> <li>Goal 2: Create and connect multiple netns inside Qemu nodes</li> </ul>"},{"location":"networking/qemu_network/qemu/#goal-1-create-and-connect-multiple-qemu-nodes","title":"Goal 1: Create and connect multiple Qemu nodes","text":""},{"location":"networking/qemu_network/qemu/#boot-qemu-node-using-alpine-image","title":"boot Qemu node using alpine image","text":"<pre><code># create virtual disk\nqemu-img create -f qcow2 alpine.qcow2 1G\n</code></pre> <pre><code># boot from iso image\n# `-boot d` means boot from cdrom\nqemu-system-x86_64 \\\n  -boot d \\\n  -cdrom alpine-standard-3.22.0-x86_64.iso \\\n  -m 512 \\\n  -hda alpine.backing.qcow2 \\\n  -nographic \\\n  -enable-kvm\n</code></pre> <pre><code># boot from backing file\n# `-boot c` means boot from disk\nqemu-system-x86_64 \\\n  -boot c \\\n  -m 512 \\\n  -hda alpine.backing.qcow2 \\\n  -nographic \\\n  -enable-kvm\n</code></pre> <ul> <li>Note:<ul> <li>must run <code>setup-alpine</code> in order to install bootable disk</li> <li>chose <code>sda</code> -&gt; <code>sys</code> -&gt; (y) for erasing the disk</li> </ul> </li> </ul>"},{"location":"networking/qemu_network/qemu/#create-overlay-image","title":"create Overlay image","text":"<pre><code># create overlay image\nqemu-img create -f qcow2 -b alpine.backing.qcow2 -F qcow2 alpine.overlay.img\n</code></pre> <pre><code># boot from overlay image\nqemu-system-x86_64 \\\n  -m 512 \\\n  -hda alpine.overlay.img \\\n  -enable-kvm \\\n  -boot c \\\n  -nographic\n</code></pre>"},{"location":"networking/qemu_network/qemu/#using-screen-to-store-qemu-consoles","title":"using <code>screen</code> to store QEMU consoles","text":"<pre><code># create session\nscreen -S qemu\n#\n# boot from overlay\n#\n\n# detach: Ctrl + A, then D\n\n# attach: \nscreen -r qemu\n</code></pre>"},{"location":"networking/qemu_network/qemu/#connect-multiple-qemu-nodes","title":"connect multiple QEMU nodes","text":"<pre><code># create bridge\nsudo ip link add name br0 type bridge\nsudo ip addr add 192.168.100.1/24 dev br0\nsudo ip link set br0 up\n\n# config iptables for br0\niptables -I FORWARD -p all -i br0 -j ACCEPT\niptables -I INPUT -p all -i br0 -j ACCEPT\niptables -I OUTPUT -p all -o br0 -j ACCEPT\n\n# create tap device\nsudo ip tuntap add dev tap0 mode tap user $USER\nsudo ip link set tap0 master br0\nsudo ip link set tap0 up\n</code></pre> <pre><code># boot from overlay &amp; plugin the tap\nqemu-system-x86_64 \\\n  -m 512 \\\n  -hda alpine.overlay.qcow2 \\\n  -netdev tap,id=net0,ifname=tap0,script=no,downscript=no \\\n  -device e1000,netdev=net0 \\\n  -enable-kvm \\\n  -nographic\n\n# setup interface \n# Interface: eth0\n# IP: 192.168.100.10\n# Gateway: 192.168.100.1\nsetup-interfaces\n\n# assign ip \nip addr add 192.168.100.10/24 dev eth0\n</code></pre> <ul> <li>Note:<ul> <li>MAC address is auto generated</li> <li>E.g: <code>52:54:00:12:34:56</code><ul> <li>where <code>52:54:00</code> is Qemu prefix</li> </ul> </li> </ul> </li> </ul>"},{"location":"networking/qemu_network/qemu/#create-netns-node-inside-qemu","title":"create netns node inside QEMU","text":"<pre><code># another overlay image for netns\nqemu-img create -f qcow2 -b alpine.overlay.qcow2 -F qcow2 alpine.overlay.tap0.qcow2\nqemu-img create -f qcow2 -b alpine.overlay.qcow2 -F qcow2 alpine.overlay.tap1.qcow2\n</code></pre> <ul> <li>Create multiple qemu node</li> </ul> <pre><code># node 0\nqemu-system-x86_64 \\\n-m 512 \\\n-hda alpine.overlay.tap0.qcow2 \\\n-netdev tap,id=net0,ifname=tap0,script=no,downscript=no \\\n-device e1000,netdev=net0,mac=52:54:00:00:00:01 \\\n-enable-kvm \\\n-nographic\n</code></pre> <pre><code># node 1\n\n# create tap1 device\nsudo ip tuntap add dev tap1 mode tap user $USER\nsudo ip link set tap1 master br0\nsudo ip link set tap1 up\n\n# boot node 1\nqemu-system-x86_64 \\\n-m 512 \\\n-hda alpine.overlay.tap1.qcow2 \\\n-netdev tap,id=net1,ifname=tap1,script=no,downscript=no \\\n-device virtio-net-pci,netdev=net1,mac=52:54:00:00:00:02 \\\n-enable-kvm \\\n-nographic\n\n# change ip address to 192.168.100.20\nvi /etc/network/interfaces\nservice networking restart\n</code></pre> <ul> <li>Now we have 2 Qemu nodes.<ul> <li>Node 0 with IP 192.168.100.10</li> <li>Node 1 with IP 192.168.100.20</li> <li>2 Nodes connect with brigde <code>br0</code> via tap devices</li> <li>2 Nodes can ping eachother</li> </ul> </li> </ul> <pre><code># from node 0\nlocalhost:~# ip addr show eth0\n2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP 0\n    link/ether 52:54:00:00:00:01 brd ff:ff:ff:ff:ff:ff\n    inet 192.168.100.10/24 scope global eth0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::5054:ff:fe00:1/64 scope link \n       valid_lft forever preferred_lft forever\nlocalhost:~# ping 192.168.100.20\nPING 192.168.100.20 (192.168.100.20): 56 data bytes\n64 bytes from 192.168.100.20: seq=0 ttl=64 time=0.259 ms\n64 bytes from 192.168.100.20: seq=1 ttl=64 time=0.288 ms\n64 bytes from 192.168.100.20: seq=2 ttl=64 time=0.287 ms\n</code></pre> <pre><code># from node 1\nlocalhost:~# ip a show eth0\n2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP 0\n    link/ether 52:54:00:00:00:02 brd ff:ff:ff:ff:ff:ff\n    inet 192.168.100.20/24 scope global eth0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::5054:ff:fe00:2/64 scope link \n       valid_lft forever preferred_lft forever\nlocalhost:~# ping 192.168.100.20\nPING 192.168.100.20 (192.168.100.20): 56 data bytes\n64 bytes from 192.168.100.20: seq=0 ttl=64 time=0.030 ms\n64 bytes from 192.168.100.20: seq=1 ttl=64 time=0.035 ms\n64 bytes from 192.168.100.20: seq=2 ttl=64 time=0.035 ms\n</code></pre>"},{"location":"networking/qemu_network/qemu/#goal-2-create-and-connect-multiple-netns-inside-qemu-nodes","title":"Goal 2: Create and connect multiple netns inside Qemu nodes","text":""},{"location":"networking/qemu_network/qemu/#install-iproute2","title":"install iproute2","text":"<ul> <li>The iproute2 package in base ISO image does not provide <code>netns</code> option.</li> <li>So, we will download the full version of iproute2 from alpine CDN <pre><code>  # download package\n  wget https://dl-cdn.alpinelinux.org/alpine/edge/main/x86_64/libelf-0.193-r0.apk\n  wget https://dl-cdn.alpinelinux.org/alpine/edge/main/x86_64/libmnl-1.0.5-r2.apk\n  wget https://dl-cdn.alpinelinux.org/alpine/edge/main/x86_64/iproute2-minimal-6.15.0-r0.apk\n  wget https://dl-cdn.alpinelinux.org/alpine/edge/main/x86_64/ifupdown-ng-iproute2-0.12.1-r7.apk\n  wget https://dl-cdn.alpinelinux.org/alpine/edge/main/x86_64/libxtables-1.8.11-r1.apk\n  wget https://dl-cdn.alpinelinux.org/alpine/edge/main/x86_64/iproute2-tc-6.15.0-r0.apk\n  wget https://dl-cdn.alpinelinux.org/alpine/edge/main/x86_64/iproute2-ss-6.15.0-r0.apk\n  wget https://dl-cdn.alpinelinux.org/alpine/edge/main/x86_64/iproute2-6.15.0-r0.apk\n</code></pre></li> </ul> <pre><code># move to Node0\nscp *.apk root@192.168.100.10:/root\n\n# \nssh root@192.168.100.10\napk add --allow-untrusted ./*.apk\n\n# now `ip netns` is ready to use\n</code></pre>"},{"location":"networking/qemu_network/qemu/#create-netns","title":"create netns","text":"<ul> <li>Setting up netns like this:</li> </ul> <pre><code># Configuration\nPHY_IF=\"eth0\"\nNET1=\"ns1\"\nNET2=\"ns2\"\nIP1=\"192.168.100.11/24\"\nIP2=\"192.168.100.12/24\"\nGW=\"192.168.100.1\"\n\n# Enable IP forwarding\necho 1 &gt; /proc/sys/net/ipv4/ip_forward\n\n# Create network namespaces\nip netns add $NET1\nip netns add $NET2\n\n# Create macvlan interfaces linked to $PHY_IF\nip link add macvlan1 link $PHY_IF type macvlan mode bridge\nip link add macvlan2 link $PHY_IF type macvlan mode bridge\n\n# Assign interfaces to namespaces\nip link set macvlan1 netns $NET1\nip link set macvlan2 netns $NET2\n\n# Bring up interfaces inside namespaces\nip netns exec $NET1 ip addr add $IP1 dev macvlan1\nip netns exec $NET2 ip addr add $IP2 dev macvlan2\nip netns exec $NET1 ip link set dev macvlan1 up\nip netns exec $NET2 ip link set dev macvlan2 up\n\n# loopback up\nip netns exec $NET1 ip link set lo up\nip netns exec $NET2 ip link set lo up\n</code></pre> <ul> <li>Note 1. Why macvlan?</li> <li>by default, netns does not have a MAC address</li> <li>So, we create a macvlan interface and move it into netns</li> <li>Kernel give macvlan interface an unique MAC address</li> <li>macvlan interface is bond to <code>eth0</code> where eth0 is <code>tap0</code>, and <code>tap0</code> has <code>br0</code> as master</li> <li> <p>The flow of layer 2 is: netns -&gt; macvlan interface -&gt; <code>tap0</code> -&gt; <code>br0</code></p> </li> <li> <p>Now the 2 netns can ping each other:</p> </li> </ul> <pre><code>localhost:~# ip netns exec ns1 ping 192.168.100.12\nPING 192.168.100.12 (192.168.100.12): 56 data bytes\n64 bytes from 192.168.100.12: seq=0 ttl=64 time=0.033 ms\n64 bytes from 192.168.100.12: seq=1 ttl=64 time=0.040 ms\n</code></pre> <ul> <li> <p>The flow of ICMP package:   </p> </li> <li> <p>Because macvlan allow each netns has it's own MAC address     So, bridge <code>br0</code> can catch ethenet package send from <code>ns1</code> and forward it to <code>ns2</code></p> </li> <li> <p>Using netcat to test the connection between 2 netns.</p> <ul> <li>Using tcpdump to see the package is sent to <code>br0</code> (192.168.100.1)   </li> </ul> </li> </ul>"},{"location":"networking/qemu_network/qemu/#create-and-connect-netnss-from-multiple-qemu-nodes","title":"create and connect netns(s) from multiple Qemu nodes","text":"<ul> <li>On Qemu Node2, create 2 netns the same way as we did on Qemu Node1</li> <li>Remember to change IP address of netns  to <code>.21</code> and <code>.22</code></li> </ul> <ul> <li>Ping from netns inside Node2 to netns inside Node1</li> </ul> <pre><code>localhost:~# ip netns exec ns1 ip a\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n      valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host proto kernel_lo \n      valid_lft forever preferred_lft forever\n5: macvlan1@if2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000\n    link/ether 7a:9c:73:31:91:8e brd ff:ff:ff:ff:ff:ff link-netnsid 0\n    inet 192.168.100.21/24 scope global macvlan1\n      valid_lft forever preferred_lft forever\n    inet6 fe80::789c:73ff:fe31:918e/64 scope link proto kernel_ll \n      valid_lft forever preferred_lft forever\nlocalhost:~# ip netns exec ns1 ping 192.168.100.11\nPING 192.168.100.11 (192.168.100.11): 56 data bytes\n64 bytes from 192.168.100.11: seq=0 ttl=64 time=0.213 ms\n64 bytes from 192.168.100.11: seq=1 ttl=64 time=0.382 ms\n64 bytes from 192.168.100.11: seq=2 ttl=64 time=0.318 \n</code></pre> <ul> <li> <p>We can also use <code>netcat</code> to let netns inside Node1 talks to netns inside Node2</p> </li> <li> <p>This setup allow the cluster to be able to scale-up</p> </li> </ul>"},{"location":"networking/tunnel/tunnel/","title":"Tunnel","text":""},{"location":"networking/tunnel/tunnel/#encapsulate-tunnel","title":"Encapsulate Tunnel","text":"<ul> <li> <p>Just a simple interface which:</p> <ul> <li>Wrap a package into a bigger package</li> </ul> <p></p> <ul> <li> <p>The purpose is to change the source IP and destination IP</p> <ul> <li> <p>The bigger package will enter the linux routing subsystem (ipfilter, iptable, ...)</p> </li> <li> <p>and receive a proper route coresponse with new source IP and destination IP</p> </li> </ul> </li> </ul> </li> <li> <p>Note:</p> <ul> <li> <p>Some people thing creating a tunnel is kind of creating a new Layer 3 path. Because when they type ifconfig and see tunnel interface same as eth0 interface</p> </li> <li> <p>But, it's not true</p> </li> <li> <p>Tunnel simply changes the src/des IP. And then, kernel will find a different route for the new bigger package</p> </li> </ul> </li> </ul>"},{"location":"programming_concept/conditional_variable/conditional_variable/","title":"Conditional Variable","text":""},{"location":"programming_concept/conditional_variable/conditional_variable/#defination","title":"Defination","text":"<ul> <li> <p>A Synchronization primative.</p> <ul> <li>Alow thread to wait for a specific condition</li> <li>Thread goes to sleep when waiting</li> </ul> </li> <li> <p>What is the difference between ConditionVariable vs Mutext?</p> <ul> <li>Mutext:<ul> <li>Thread waits for Mutex to be unlocked</li> </ul> </li> <li>ConditionVariable:<ul> <li>Thread waits for a notification</li> </ul> </li> </ul> </li> <li> <p>The difference is seem not much clear. Let's take an example of a channel which will be used by multiple threads.</p> </li> <li> <p>The channel consits of a Queue to hold a shared data.</p> <ul> <li>A thread can push data to the channel.</li> <li>Another thread can read data from channel.</li> </ul> </li> <li> <p>We can think of using mutex to protect the shared data. At a given point of time, only 1 thread can have access and modify the shared data.</p> </li> </ul> <p></p> <ul> <li>The 1st design has a problem:<ul> <li>The consume thread has no way to know if the channel is empty or not.</li> <li>So, consume thread might try to acquire the Mutex lock for nothing (in case queue is empty),</li> <li>And, consume thread also has to release the Mutex lock, so the produce thread can push data in queue.</li> <li>It means, the consume thread has to acquire the lock, see nothing in the queue, and release it, and then re-acquire, ...</li> </ul> </li> </ul> <p></p> <ul> <li> <p>Conditional Variable comes to the rescue. Let's imagine the scenario:</p> Consume thread Produce thread acquire Mutex lock see nothing in queue setups a Conditional Variable Release mutex goes to sleep acquire Mutex lock push data on queue release Mutex lock use the Conditional Variable to notify to Consume Thread wakeup by Conditional Variable acquire Mutex lock read from queue </li> </ul> <p></p>"},{"location":"programming_concept/dependency_injection/dependency_injection/","title":"Dependency Injection","text":"<p>This is about to explain the Dependency Injection (DI) in a simple way. Oviously, it's a simple thing, don't make it complex !!!</p>"},{"location":"programming_concept/dependency_injection/dependency_injection/#what-is-di","title":"What is DI","text":"<ul> <li> <p>DI means:</p> <ul> <li>Give an Object something.</li> </ul> </li> <li> <p>Just simple like this. For e.g, </p> <ul> <li>An Object does not create it's own Database </li> <li>We will choose a Database then give it to the Object</li> </ul> </li> </ul>"},{"location":"programming_concept/dependency_injection/dependency_injection/#without-di","title":"Without DI","text":"<ul> <li>Object creates then hold a Database connection:</li> </ul>"},{"location":"programming_concept/dependency_injection/dependency_injection/#with-di","title":"With DI","text":"<ul> <li> <p>Pass the Database connection to Object via contruction.</p> </li> <li> <p>Or: Use get/set func.</p> </li> <li> <p>The goal is: we can flexibly change the type of database.</p> </li> </ul> <p></p>"},{"location":"rust/async/async/","title":"Async Await","text":""},{"location":"rust/async/async/#overview","title":"Overview","text":"<ul> <li>To get a better understand about async-await, we should get to understand about concurrency programming.</li> </ul>"},{"location":"rust/async/async/#concurrency-vs-paralellism","title":"Concurrency vs Paralellism","text":"<ul> <li> <p>Paralellism &amp; Concurrency is two approachs to handle multiple tasks at the same time.</p> </li> <li> <p>Paralellism:</p> <ul> <li>Each thread handle 1 task.</li> <li>Use multiple threads to handle multiple tasks.</li> </ul> </li> <li> <p>Concurrency:</p> <ul> <li>Each thread can handle multiple tasks -&gt;&gt; no need much thread.</li> <li>At a point of time, one thread can execute only 1 task, but the thread can switch between it's tasks.</li> </ul> <p></p> </li> <li> <p>Comments &amp; Questions:</p> <ul> <li>In both approachs, one thread can execute 1 task at a time. Why use Concurrency over Paralellism ?</li> <li>In the image, it's seem Paralellism can finish the Green Task faster than Concurrency ?</li> <li>Paralellism can isolated task. It looks much better.</li> <li>How Concurrency knows how to mix the task ?</li> </ul> </li> <li> <p>The answers for those questions above is inside the task.     Concurrency will show it's advantage when work with the tasks which have to wait for events from I/O.     The CPU core runs much faster than I/O devices. So, for e.g, it has to wait for I/O drivers to write something to disk, or wait for I/O socket, ...etc.</p> <p></p> </li> <li> <p>Idle CPU core is a waste of resource.</p> <ul> <li>In Paralellism, thread must yeild the CPU to another thread, and let the kernel schedule to have chance to run again.     Ofcourse, the next thread might belong to another program.</li> <li>In Concurrency, NO yeilding happends. Thread will switch to another task, if current task are waiting for I/O.</li> </ul> <p></p> </li> <li> <p>The image shows that the Green Task might be completed earlier, compare to Paralellism.</p> </li> <li> <p>But, Paralellism is easier to implement, because Kernel helps to take care all the threads.</p> </li> <li> <p>To implement Concurrency, we need to implement mechanism for thread to be able to hold tasks and switch between tasks. It's time for Runtime to comes as a rescuer.</p> </li> </ul>"},{"location":"rust/async/async/#runtime","title":"Runtime","text":"<ul> <li> <p>The most popular runtime for Rust is Tokio runtime</p> </li> <li> <p>Runtime provide mechanisms for:</p> <ul> <li>Store list of tasks run in concurrency</li> <li>Which task will be run next</li> <li>When to re-invoke a pending task</li> <li>I/O functions for async programming ...</li> </ul> <pre><code>// example from https://tokio.rs/tokio/tutorial/spawning\nuse tokio::net::TcpListener;\n\n#[tokio::main]\nasync fn main() {\n    let listener = TcpListener::bind(\"127.0.0.1:6379\").await.unwrap();\n\n    loop {\n        let (socket, _) = listener.accept().await.unwrap();\n        // A new task is spawned for each inbound socket. The socket is\n        // moved to the new task and processed there.\n        tokio::spawn(async move {\n            process(socket).await;\n        });\n    }\n}\n</code></pre> </li> <li> <p>Note for #[tokio::main]</p> <pre><code>#[tokio::main]\nasync fn main() {\n    code_inside_main_fn();\n}\n\n// is equivalent to\n\nfn main() {\n    let mut rt = tokio::runtime::Runtime::new().unwrap();\n    rt.block_on(async {\n        code_inside_main_fn();\n    })\n}\n</code></pre> </li> <li> <p>Tokio run time has 2 operation mode:</p> <ul> <li>Single-thread runtime: all tasks, executor, reactor are placed inside 1 thread.</li> <li>Multi-thread runtime: using multiple thread to execute tasks (thread pool)</li> </ul> <p></p> </li> <li> <p>Bellow is how a task is randomly moved to and be executed in a thread.</p> </li> <li> <p>The mechanism to drive I/O events is based on Operating System and the system call must be Non-Blocking.</p> <ul> <li>Linux: epoll</li> <li>Mac: kqueue</li> <li>Windows: IOCP Input/output completion port</li> </ul> <p></p> </li> <li> <p>In Linux, epoll can be use to ask kernel to mornitor for a I/O, which represented by a File Description (fd) number.</p> <ul> <li>Epoll is non-blocking, the result is return immediately.</li> </ul> <p></p> </li> </ul>"},{"location":"rust/async/async/#async-await_1","title":"Async Await","text":"<ul> <li> <p>Async Await key words are use to write asynchronous task.</p> <pre><code>async fn main {\n    asynchronous_task.await();\n}\n\n\nasync fn asynchronous_task () -&gt; uszie {\n    println!{\"hello world\"};                    // work\n    let n_char = read_from_disk().await();      // wait ...\n    return n_char                               // work\n}\n//  is equivalent with \nfn asynchronous_task () -&gt; impl Future&lt;Output = usize&gt; {\n    async {\n        // ...\n    } \n}\n</code></pre> </li> <li> <p>Async function is somehow equipvalent with a Future. Let's find out what is Future.</p> </li> <li> <p>Future is defined as:</p> <p>A future is a value that might not have finished computing yet. This kind of \u201casynchronous value\u201d makes it possible for a thread to continue doing useful work while it waits for the value to become available.</p> </li> <li> <p>The Future's defination matchs with the waiting part in our task above.</p> <p></p> </li> <li> <p>The core method of future, poll, attempts to resolve the future into a final value</p> </li> <li> <p>We can write a poll funtion in a simplest way:</p> <ul> <li>call read() with O_NON_BLOCKING flag, it means, check with kernel if a FD is ready to read.</li> <li>if fd is ready, the data is return from read(), future is done.</li> <li>if fd is not ready, read() returns WOULD_BLOCK -&gt;&gt; ask waker to monitor the fd.</li> <li>the poll will be called again, when waker signals executor.</li> </ul> <pre><code>// simplest future\nfn poll(waker) {\n\n    let r = read(fd, ops, O_NON_BLOCKING);\n\n    if r == OK {\n        return r.data;\n    }\n\n    if r == WOULD_BLOCK {\n        wake.push(fd, ops);\n    }\n}\n</code></pre> </li> <li> <p>How to write a complex Future.</p> <ul> <li>And how the poll() function knows where to continue for the 2nd poll.</li> </ul> <pre><code>// nested poll function\nfn poll () {\n    // work before read\n    // future 1\n    fut_read_socket.await();\n    // work before write\n    // future 2\n    fut_write_socket.await();\n}\n</code></pre> </li> <li> <p>Actually, before the introduction of async-await, async task was implemented as a State Machine.</p> <pre><code>Enum State&lt;T&gt; {\n    State_Start,\n    State_Reading(Option&lt;T&gt;),    // before read() completed\n    State_Writing(x, y, z),     // before write() completed\n    State_End,\n}\n\nimpl&lt;T&gt; Future for State {\n    type Output = ();\n\n    fn poll(&amp;mut self) -&gt; Poll&lt;Self::Output&gt; {\n\n        match self {\n\n            State::State_Start =&gt; {\n                println!{\"some work before Read\"};\n\n                // change state\n                *self = State::State_Reading(data, ...);\n                return self.poll();\n            }\n\n            State::State_Reading(data, ...) =&gt; {\n\n                if let Poll::Ready() = fut_read_socket.poll() {\n\n                    println!{\"some work before Write\"};\n\n                    // change state\n                    *self = State::State_Writing(data, ...);\n                    return self.poll();\n                }\n                else {\n                    Poll::Pending\n                }\n            }\n\n        State::State_Writing(data, ...) =&gt; {\n\n                if let Poll::Ready() = fut_write_socket.poll() {\n\n                    // change state\n                    *self = State::State_End;\n                    return self.poll();\n                }\n                else {\n                    Poll::Pending\n                }\n            }\n\n            State::State_End =&gt; {\n                Poll::Ready(())\n            }\n\n        }\n    }\n}\n</code></pre> </li> <li> <p>By using state machine, we can ensure that we can call poll() function multiple time but some logic is not be executed more than once.</p> <p></p> </li> <li> <p>The detail of how state machine work</p> <p></p> <p></p> <p></p> </li> </ul>"},{"location":"rust/axum/FromRef/","title":"FromRef","text":""},{"location":"rust/axum/FromRef/#rationale","title":"Rationale","text":"<ul> <li> <p>When using <code>Axum</code>, let's declare a <code>State</code> like:</p> <p><pre><code>#[derive(Clone)]\nstruct AppState {\n    db: Database,\n    cf: Config,\n}\n</code></pre> * Now, suppose one of your handlers only needs access to <code>db</code>. You could store separate <code>Arc&lt;Database&gt;</code> and <code>Arc&lt;Config&gt;</code> in the router, but that\u2019s messy.</p> <p></p> </li> <li> <p>Instead, you can keep a single shared state <code>AppState</code> and tell <code>Axum</code> how to get <code>references</code> to parts of it \u2014 this is where <code>FromRef</code> comes in.</p> </li> </ul>"},{"location":"rust/axum/FromRef/#solution","title":"Solution","text":"<ul> <li> <p>Using Axum's macro <code>FromRef</code> allows a handler to extract a part of <code>AppState</code> in a way that look so easy.</p> <p></p> </li> <li> <p>So, how does this magic work under the hood?</p> </li> </ul>"},{"location":"rust/axum/FromRef/#how-fromref-is-implemented","title":"How FromRef is implemented","text":"<ul> <li> <p>The key point is if a handler has an <code>AppState</code> \u2014 how can it turn that into a <code>Database</code>?</p> </li> <li> <p>That\u2019s where this trait comes in:     <pre><code>pub trait FromRef&lt;T&gt; {\n    fn from_ref(input: &amp;T) -&gt; Self;\n}\n</code></pre></p> </li> <li> <p>De-sugar macro</p> <pre><code>#[derive(Clone)]\nstruct AppState {\n    db: Database,\n    ...\n}\n\n#[derive(Clone)]\nstruct Database;\n\nimpl FromRef&lt;AppState&gt; for Database {\n    fn from_ref(app_state: &amp;AppState) -&gt; Database {\n        app_state.db.clone()\n    }\n}\n</code></pre> </li> <li> <p>Extract state:     <pre><code>// pseudo - called inside State extractor\nfn extract_state(appState) -&gt; Database {\n    db = Database::from_ref(&amp;app_state);\n}\n</code></pre></p> </li> <li> <p>Axum uses <code>.clone()</code> when extracting subfields. That\u2019s why all <code>AppState</code> fields must implement <code>Clone</code></p> </li> </ul>"},{"location":"rust/axum/FromRef/#nested-fromref","title":"Nested FromRef","text":"<ul> <li> <p>Imagine you have nested layers of state:</p> <pre><code>AppState\n\u251c\u2500\u2500 Database\n\u2502    \u2514\u2500\u2500 Cache\n\u2514\u2500\u2500 Config\n</code></pre> </li> <li> <p>Axum can extract <code>Cache</code> in-directly, if you manually chain <code>FromRef</code> implementations:</p> <ul> <li>Database can be built from AppState.</li> <li>[Manually add implementation for] Cache can be built from Database. <pre><code>cache = Cache::from_ref(&amp;Database::from_ref(&amp;AppState))\n</code></pre></li> </ul> </li> <li> <p>Generated by Macro     <pre><code>// 1\ufe0f\u20e3 Database from AppState\nimpl FromRef&lt;AppState&gt; for Database {\n    fn from_ref(app_state: &amp;AppState) -&gt; Database {\n        app_state.db.clone()\n    }\n}\n</code></pre></p> </li> <li> <p>Manually wirtten by you     <pre><code>// 2\ufe0f\u20e3 Cache from Database\nimpl FromRef&lt;Database&gt; for Cache {\n    fn from_ref(db: &amp;Database) -&gt; Cache {\n        db.cache.clone()\n    }\n}\n</code></pre></p> </li> <li> <p>Now Axum can derive Cache from AppState automatically! Because it knows <code>Database::from_ref(AppState)</code> and <code>Cache::from_ref(Database)</code>.</p> <pre><code>async fn handler(State(cache): State&lt;Cache&gt;) {\n    println!(\"Cache size: {}\", cache.size);\n}\n</code></pre> </li> </ul>"},{"location":"rust/axum/axum/","title":"Axum","text":""},{"location":"rust/axum/axum/#architecture-overview-just-what-i-know","title":"Architecture overview (just what I know)","text":"<ul> <li> <p>Axum is a Rust crate for building a web server.</p> </li> <li> <p>Axum consist of:</p> <ul> <li>hyper for handle http protocol Or, Tonic for gRPC</li> <li>Tokio as runtime for async tasks</li> <li>matchit for matching a request to a Route</li> <li>tower for middleware</li> </ul> </li> <li> <p>Actually, tower plays a very important role in axum.</p> </li> <li> <p>Here is the brieft overview of how a request flow inside Axum:</p> <p></p> </li> <li> <p>To make it more easier to understand:</p> <p></p> </li> </ul> <pre><code>// example from: https://crates.io/crates/axum\n#[tokio::main]\nasync fn main() {\n\n    let app = Router::new()\n        .route(\"/\", get(root))\n        .route(\"/users\", post(create_user));\n\n    // run our app with hyper, listening globally on port 3000\n    let listener = tokio::net::TcpListener::bind(\"0.0.0.0:3000\").await.unwrap();\n    axum::serve(listener, app).await.unwrap();\n}\n\n// handler: root\nasync fn root() -&gt; &amp;'static str {\n    \"Hello, World!\"\n}\n\n// handler: create_user\nasync fn create_user(\n    Json(payload): Json&lt;CreateUser&gt;,\n) -&gt; (StatusCode, Json&lt;User&gt;) {\n\n    (StatusCode::CREATED, Json(user))\n\n}\n</code></pre> <ul> <li> <p>Qus: I do not see Service in the above code?</p> <ul> <li>Ans: because Axum use Middleware or Handler which eventually be converted to Service</li> </ul> </li> </ul>"},{"location":"rust/axum/axum/#tower-service","title":"Tower Service","text":"<ul> <li> <p>Trait Service:</p> <ul> <li>Turn Request -&gt;&gt; Response <pre><code>pub trait Service&lt;Request&gt; {\n\n    // back pressure\n    fn poll_ready(\n        &amp;mut self,\n        cx: &amp;mut Context&lt;'_&gt;,\n    ) -&gt; Poll&lt;Result&lt;(), Self::Error&gt;&gt;;\n\n    type Future: Future&lt;Output = Result&lt;Self::Response, Self::Error&gt;&gt;;\n\n    fn call(&amp;mut self, req: Request) -&gt; Self::Future;\n}\n</code></pre></li> </ul> </li> <li> <p>Trait Layer:</p> <ul> <li>To stack up Services:</li> </ul> </li> </ul> <pre><code>pub trait Layer&lt;S&gt; {\n    type Service;\n\nfn layer(&amp;self, inner: S) -&gt; Self::Service;\n}\n</code></pre> <p></p> <pre><code>* Refer: [Inventing the Service trait](https://tokio.rs/blog/2021-05-14-inventing-the-service-trait \"tokio.rs\")\n</code></pre>"},{"location":"rust/axum/axum/#middleware","title":"Middleware","text":"<pre><code>let routes_all = Router::new()\n    .nest(\"/api\", routes_apis)\n    .layer(middleware::map_response(logging))\n    .layer(middleware::from_fn_with_state(\n        app_state.clone(), token,\n    ))\n    .layer(CookieManagerLayer::new())\n\n\npub async fn token(mut req: Request&lt;Body&gt;, next: Next) -&gt; Result&lt;Response&gt; {\n    // do smt\n    Ok(next.run(req).await)\n}\n\n\nasync fn logging(res: Response) -&gt; Response {\n    // do smt\n    res\n}\n</code></pre>"},{"location":"rust/axum/axum/#handler","title":"Handler","text":"<ul> <li> <p>Handler:</p> <ul> <li> <p>take extractors to parse Request</p> </li> <li> <p>return sth that can be converted into a Response</p> </li> </ul> </li> <li> <p>2 Types of Handler:</p> <ul> <li> <p>With STATE</p> </li> <li> <p>Without STATE</p> </li> </ul> </li> </ul> <p></p> <ul> <li> <p>Qus: Does Axum accept all func as Handler?</p> <ul> <li> <p>Ans 1: handler must be:</p> <ul> <li> <p>Async func to return future</p> </li> <li> <p>take 0 -&gt;&gt; 16 extractors</p> </li> <li> <p>the request's body part should be extracted at the last extractor </p> </li> </ul> </li> <li> <p>Ans 2: suitable func will be automatically imlp Handler trait by the macro all_the_tuples</p> </li> </ul> </li> </ul> <pre><code>#[rustfmt::skip]\nmacro_rules! all_the_tuples {\n    ($name:ident) =&gt; {\n        $name!([], T1);\n        $name!([T1], T2);\n        $name!([T1, T2], T3);\n        $name!([T1, T2, T3], T4);\n        $name!([T1, T2, T3, T4], T5);\n        $name!([T1, T2, T3, T4, T5], T6);\n        $name!([T1, T2, T3, T4, T5, T6], T7);\n        $name!([T1, T2, T3, T4, T5, T6, T7], T8);\n        $name!([T1, T2, T3, T4, T5, T6, T7, T8], T9);\n        $name!([T1, T2, T3, T4, T5, T6, T7, T8, T9], T10);\n        $name!([T1, T2, T3, T4, T5, T6, T7, T8, T9, T10], T11);\n        $name!([T1, T2, T3, T4, T5, T6, T7, T8, T9, T10, T11], T12);\n        $name!([T1, T2, T3, T4, T5, T6, T7, T8, T9, T10, T11, T12], T13);\n        $name!([T1, T2, T3, T4, T5, T6, T7, T8, T9, T10, T11, T12, T13], T14);\n        $name!([T1, T2, T3, T4, T5, T6, T7, T8, T9, T10, T11, T12, T13, T14], T15);\n        $name!([T1, T2, T3, T4, T5, T6, T7, T8, T9, T10, T11, T12, T13, T14, T15], T16);\n    };\n}\n</code></pre> <ul> <li> <p>Qus: Handler is a Service ?</p> <ul> <li>Ans: No. But a Handler will be converted into a Service Converting Handlers into Services</li> </ul> </li> <li> <p>Qus: How extractor work?    </p> <ul> <li> <p>Ans:  impl_handler </p> </li> <li> <p>Refer for more: extractor</p> </li> </ul> </li> <li> <p>Qus: How IntoResponse work?</p> <ul> <li>Ans: impl_into_response</li> </ul> </li> <li> <p>Custom Extractor:</p> </li> </ul> <pre><code>struct ExtData {}\n\nimpl &lt;S: Send + Sync&gt; FromRequestParts&lt;S&gt; for ExtData {\n\n    async fn from_request_parts(parts, _state) -&gt; Result&lt;Self&gt; {\n        // extract data from parts &amp; put to Self (ExtData)\n    }\n}\n</code></pre>"},{"location":"rust/axum/axum/#state","title":"State","text":"<ul> <li> <p>State is shared mutual data between services (handler/miiddleware)</p> </li> <li> <p>E.g: </p> <ul> <li>2 Requests are handled by 2 Handlers.</li> <li>The 2 Handlers both try to get the entry to Database from State.</li> <li>So, the State is a kind of global data, all services has access to State.</li> </ul> </li> <li> <p>Add State to Router:</p> </li> </ul> <pre><code>Struct AppState {\n    // ...\n}\n\nlet app_state = AppState {...}\n\nlet routes_all = Router::new()\n    .nest(\"/api\", routes_apis)\n    .layer(middleware::map_response(logging))\n    .layer(middleware::from_fn_with_state(\n        app_state.clone(), token,\n    ))\n    .layer(CookieManagerLayer::new())\n    .with_state(app_state)\n    ;\n</code></pre> <ul> <li>Add State Extractor to Handler:</li> </ul> <pre><code>async fn handler_with_state_extractor(\n    State(state) : State&lt;AppState&gt;,\n) -&gt; Result&lt;&gt; {\n    // handler logic\n}\n</code></pre> <ul> <li> <p>How state is passed to extractor: <pre><code>// get() return MethodRouter\npub fn get&lt;H, T, S&gt;(handler: H) -&gt; MethodRouter&lt;S, Infallible&gt;\n\n// MethodRouter call to Route. with State=() ???\nimpl&lt;B&gt; Service&lt;Request&lt;B&gt;&gt; for Router&lt;()&gt;\n\n// Route pass STATE to handler\npub trait Handler&lt;T, S&gt;\n\n// Handler pass STATE to Extractor\npub trait FromRequestParts&lt;S&gt;\n</code></pre></p> </li> <li> <p>Qus: Route take State=(). What STATE actually passed to Handler?</p> <ul> <li>Ans: <ul> <li>Actually, the concrete type of STATE is inferred from the State Extractor</li> <li>The moment you call the with_state(), all Handlers so far are converted to Service right away, with the concrete State type</li> </ul> </li> </ul> </li> </ul> <pre><code>pub fn with_state&lt;S2&gt;(self, state: S) -&gt; Router&lt;S2&gt;\n</code></pre>"},{"location":"rust/axum/axum/#back-pressure","title":"Back pressure","text":"<ul> <li>Some Service has a limited Capacity -&gt;&gt; might has back pressure</li> </ul> <ul> <li> <p>use poll_ready() to check the capacity first.</p> <ul> <li> <p>E.g: </p> <ul> <li> <p>If DB pool is full, return PENDDING -&gt;&gt; the request is pendding</p> </li> <li> <p>If DB pool is avaiable, connecto DB and call next Service</p> </li> </ul> </li> </ul> </li> </ul> <p></p>"},{"location":"rust/axum/extractor/","title":"Extractor","text":"<ul> <li> <p>The Path extractor can be used as simple like this.</p> <pre><code>async fn hello_path(Path(name): Path&lt;String&gt;) -&gt; impl IntoResponse {\n}\n</code></pre> </li> <li> <p>But, how?</p> </li> <li> <p>Answer 1:</p> <ul> <li> <p>Extractor actually is a data-struct.</p> </li> <li> <p>Path(name) is a destructure. Destruct a Path to get a String</p> </li> <li> <p>Example: <pre><code>struct Foo {\n    x: u32,\n    y: u32,\n}\nlet faa = Foo { x: 1, y: 2 };\n\n// destruct faa -&gt; x0, y0\nlet Foo { x : x0, y: y0 } = faa;\nprintln!(\"Outside: x0 = {x0:?}, y0 = {y0}\");\n</code></pre></p> </li> </ul> </li> <li> <p>Answer 2:</p> <ul> <li> <p>Extractor must implement either FromRequestParts or FromRequest</p> </li> <li> <p>When handler function is executed:</p> </li> </ul> <pre><code>    async fn custom_handler(extractor_1, extractor_2, ..., last_extractor) {\n        //\n        // At this point, Path(name) is init with name=\"\"\n        extractor_1.from_request_parts(request);\n        // At this point name=\"path/from/request\"\n\n        extractor_2.from_request_parts(request);\n        ...\n        last_extractor.from_request(request);\n\n        //\n        handler_logic();\n\n        //\n        (into_respon_tupple).into_response()\n    }\n</code></pre> <ul> <li> <p>Reference source code:</p> <p>impl_handler </p> </li> </ul> </li> </ul>"},{"location":"rust/axum/handler/","title":"Handler","text":""},{"location":"rust/axum/handler/#what-is-handler","title":"What is Handler","text":"<ul> <li>Arcording to Axum doccument, Handler is:<ul> <li>An async function</li> <li>take zero or more extractors</li> <li>return sth that can be converted into a Response</li> </ul> </li> </ul>"},{"location":"rust/axum/handler/#extractor","title":"Extractor","text":"<ul> <li> <p>Extractor is a type which implements FromRequestParts</p> </li> <li> <p>There are several usefull extractors:</p> <ul> <li>Params</li> <li>Path</li> <li>Json</li> <li>State</li> </ul> </li> <li> <p>We can also write our custom extractor:</p> </li> </ul>"},{"location":"rust/axum/handler/#how-handler-takes-various-amount-of-extractor","title":"How Handler takes various amount of Extractor","text":"<ul> <li> <p>Axum use a macro like all_the_tuples to automatically implement Handler trait for any function which:</p> <ul> <li> <p>take zero -&gt;&gt; 16 extractors. </p> </li> <li> <p>The last extractor must implement FromRequest trait</p> </li> <li> <p>The other extractor must implement FromRequestParts trait</p> </li> </ul> </li> <li> <p>The last extractor can be used to extract the Body part of a request. Or, you must place the body extractor as the last argument of a Handler.</p> </li> </ul> <pre><code>#[rustfmt::skip]\nmacro_rules! all_the_tuples {\n    ($name:ident) =&gt; {\n        $name!([], T1);\n        $name!([T1], T2);\n        $name!([T1, T2], T3);\n        $name!([T1, T2, T3], T4);\n        $name!([T1, T2, T3, T4], T5);\n        $name!([T1, T2, T3, T4, T5], T6);\n        $name!([T1, T2, T3, T4, T5, T6], T7);\n        $name!([T1, T2, T3, T4, T5, T6, T7], T8);\n        $name!([T1, T2, T3, T4, T5, T6, T7, T8], T9);\n        $name!([T1, T2, T3, T4, T5, T6, T7, T8, T9], T10);\n        $name!([T1, T2, T3, T4, T5, T6, T7, T8, T9, T10], T11);\n        $name!([T1, T2, T3, T4, T5, T6, T7, T8, T9, T10, T11], T12);\n        $name!([T1, T2, T3, T4, T5, T6, T7, T8, T9, T10, T11, T12], T13);\n        $name!([T1, T2, T3, T4, T5, T6, T7, T8, T9, T10, T11, T12, T13], T14);\n        $name!([T1, T2, T3, T4, T5, T6, T7, T8, T9, T10, T11, T12, T13, T14], T15);\n        $name!([T1, T2, T3, T4, T5, T6, T7, T8, T9, T10, T11, T12, T13, T14, T15], T16);\n    };\n}\n</code></pre> <pre><code>macro_rules! impl_handler {\n    (\n        [$($ty:ident),*], $last:ident\n    ) =&gt; {\n        #[allow(non_snake_case, unused_mut)]\n        impl&lt;F, Fut, S, Res, M, $($ty,)* $last&gt; Handler&lt;(M, $($ty,)* $last,), S&gt; for F\n        where\n            F: FnOnce($($ty,)* $last,) -&gt; Fut + Clone + Send + Sync + 'static,\n            Fut: Future&lt;Output = Res&gt; + Send,\n            S: Send + Sync + 'static,\n            Res: IntoResponse,\n            $( $ty: FromRequestParts&lt;S&gt; + Send, )*\n            $last: FromRequest&lt;S, M&gt; + Send,\n        {\n            type Future = Pin&lt;Box&lt;dyn Future&lt;Output = Response&gt; + Send&gt;&gt;;\n\n            fn call(self, req: Request, state: S) -&gt; Self::Future {\n                Box::pin(async move {\n                    let (mut parts, body) = req.into_parts();\n                    let state = &amp;state;\n\n                    $(\n                        let $ty = match $ty::from_request_parts(&amp;mut parts, state).await {\n                            Ok(value) =&gt; value,\n                            Err(rejection) =&gt; return rejection.into_response(),\n                        };\n                    )*\n\n                    let req = Request::from_parts(parts, body);\n\n                    let $last = match $last::from_request(req, state).await {\n                        Ok(value) =&gt; value,\n                        Err(rejection) =&gt; return rejection.into_response(),\n                    };\n\n                    let res = self($($ty,)* $last,).await;\n\n                    res.into_response()\n                })\n            }\n        }\n    };\n}\n</code></pre>"},{"location":"rust/clone/derive_clone/","title":"#[Derive Clone]","text":""},{"location":"rust/clone/derive_clone/#what-the-macro-does","title":"What the macro does","text":"<ul> <li>Automatically implement Clone trait for a type</li> </ul>"},{"location":"rust/clone/derive_clone/#implementation-for-normal-type","title":"Implementation for normal type","text":"<ul> <li> <p>What we write: <pre><code>#[derive(Clone)]\nstruct Foo {\n    a: u32\n}\n</code></pre></p> </li> <li> <p>can be translated to:</p> </li> </ul> <pre><code>impl Clone for Foo {\n  fn clone(&amp;self) -&gt; Self {\n    Foo {\n      a: self.a.clone(),\n    }\n  }\n}\n</code></pre>"},{"location":"rust/clone/derive_clone/#implementation-for-generric-type","title":"Implementation for Generric type","text":"<ul> <li>What we write:</li> </ul> <pre><code>#[derive(Clone)]\nstruct Foo &lt;T&gt; {\n    a: Rc&lt;T&gt;\n}\n</code></pre> <ul> <li>can be translated to:</li> </ul> <pre><code>impl&lt;T: ::core::clone::Clone&gt; ::core::clone::Clone for Foo&lt;T&gt; {\n  #[inline]\n  fn clone(&amp;self) -&gt; Foo&lt;B, C&gt; {\n    match *self {\n      Foo {\n        a: ref __self_0_0,\n      } =&gt; Foo {\n        a: ::core::clone::Clone::clone(&amp;(*__self_0_0)),\n      },\n    }\n  }\n}\n</code></pre> <ul> <li>Thing to notice for generic type is that:<ul> <li>event RC\\ can guarantee to be clonable for any type T <li>but, the type T should also implement Clone too</li> <pre><code>impl&lt;T: ::core::clone::Clone&gt; ::core::clone::Clone for Foo&lt;T&gt;\n</code></pre>"},{"location":"rust/closure/closure/","title":"How Closure is implemented","text":""},{"location":"rust/closure/closure/#1-closure","title":"1. Closure","text":"<ul> <li> <p>Think of closure as lambdas in C++, or arrow function in JavaScript.</p> </li> <li> <p>Closure is a combination of a function and a set of data (captured data).</p> </li> <li> <p>Or closure is a Trait object. There are 3 Traits: Fn, FnMut, FnOnce</p> </li> </ul>"},{"location":"rust/closure/closure/#closure-implement-fn-trait","title":"Closure implement Fn trait","text":"<ul> <li>Closure which implements Fn trait uses immutable reference to captures data, that why it can be called multiple times.</li> </ul> <pre><code>let color = String::from(\"green\");\nlet print = || println!(\"color: {}\", color);  \n// print is a closure\n</code></pre> <pre><code>// desugar syntactic:\nstruct PrintClosure {\n    color: &amp; String,\n    // borrow **immutable** ownership of the real color value\n}\nimpl Fn for PrintClosure {\n    fn call(&amp;self) {\n        println!(\"color: {}\", self.color);  \n    }\n}\n</code></pre>"},{"location":"rust/closure/closure/#closure-implement-fnmut-trait","title":"Closure implement FnMut trait","text":"<ul> <li>This kind of closure use mutable reference to capture data.</li> <li>When closure is created, we can not create reference to the captured-data until the closure is removed, because Rust allows only 1 mutable reference at a moment.</li> </ul> <pre><code>let mut count = 0;\nlet mut inc = || {      // notice: inc is mutable closure\n    count += 1;\n    println!(\"count: {}\", count);\n};\n</code></pre> <pre><code>// desugar syntactic:\nstruct IncClosure {\n    count: &amp;mut i32,\n    // borrow **mutable** ownership of the count value\n}\nimpl FnMut for IncClosure {\n    fn call_mut(&amp;self) {\n        println!(\"color: {}\", self.color);  \n    }\n}\n</code></pre>"},{"location":"rust/closure/closure/#closure-implement-fnonce-trait","title":"Closure implement FnOnce trait","text":"<ul> <li> <p>This closure takes ownership of captured-data.</p> </li> <li> <p>Approach 1: Use FnOnce closure when the capture data is drop at the first call.</p> </li> </ul> <pre><code>let movable = Box::new(3);\nlet consume = || {\n    println!(\"movable: {:?}\", movable);\n    mem::drop(movable);\n};\n// from now, movable lost it's value, can not be used\n</code></pre> <pre><code>// desugar syntactic:\nstruct ConsumeClosure {\n    movable: Box&lt;i32&gt;,\n}\nimpl FnOnce for ConsumeClosure {\n    fn call_once(&amp;self) {\n        println!(\"movable: {:?}\", self.movable);\n        mem::drop(self.movable);\n        // \"drop\" causes this closure can be called only 1 time.\n    }\n}\n</code></pre> <ul> <li> <p>Approach 2: Use the FnOnce closure to capture inner data.</p> <ul> <li>For e.g: The local_string will outlive the closure returned by create_closure().</li> </ul> </li> </ul> <pre><code>fn create_closure() -&gt; impl Fn() {\n    let local_string = String::from(\"hello\");\n\n    // return a closure\n    || {\n        // compiler might create a Fn closure\n        println!(\"local: \", local_string);\n    }\n\n    // local_string is destroyed, the memory which contains \"hello\" is removed\n}\n\n// call closure here return an error\nlet created_closure = create_closure();\ncreated_closure();   // error\n</code></pre> <ul> <li>Using move to move the value to inside closure, forces closure to take ownership. Also means that compiler will create FnOnce closure</li> </ul> <pre><code>fn create_closure() -&gt; impl Fn() {\n    let local_string = String::from(\"hello\");\n\n    move || {\n        println!(\"local: \", local_string);\n    }\n}\n</code></pre> <pre><code>// desugar syntactic:\nstruct AnonymousClosure {\n    local_string: String,\n    // local_string will be move to AnonymousClosure.local_string\n}\n</code></pre> <p></p>"},{"location":"rust/database/database/","title":"Database","text":""},{"location":"rust/database/modql/","title":"Modern SQL","text":""},{"location":"rust/database/seaquerry/","title":"Sea-Querry","text":""},{"location":"rust/database/sqlx/","title":"SQLX","text":""},{"location":"rust/enums/enums/","title":"Enums","text":"<p>Enum in Rust is not just a number that we used to know in some programming languages like C/C++, Java, Js, ...</p> <p>In Rust, enum is Tagged-Union which provides more usages than just a number.</p> <p>Let's take a look at what is tagged-union, then discover some extra usages that Rust provides with Enum.</p>"},{"location":"rust/enums/enums/#1-enum-in-cc","title":"1. Enum in C/C++","text":""},{"location":"rust/enums/enums/#11-enum","title":"1.1. Enum","text":"<p>Each enum value is just a 4 bytes integer value. It's often used to mark a number with a more meaning full text.</p> <pre><code>enum Level {\n  LOW,      // 0\n  MEDIUM,   // 1\n  HIGH      // 2\n};\n</code></pre>"},{"location":"rust/enums/enums/#12-union","title":"1.2. Union","text":"<p>Union value can be different data types.  Each value has size of the biggest union member.</p> <pre><code>union CarName {\n    char charName,  // use a character as a name\n    int intName,    // use a number as a name\n    char *strName,  // use a string as a name\n}\n</code></pre> <p>Union is a kind of dangerous feature in C. Because we must remember the data type.  For example:</p> <pre><code>    union CarName my_car;\n    print(\"Car name: %s\", my_car.intName); // incorrect\n</code></pre>"},{"location":"rust/enums/enums/#13-tagged-union","title":"1.3. Tagged union","text":"<p>Tagged union is like a combination of Enum and Union.</p> <pre><code>struct TaggedUnion {\n\n    int tag;            // indicate the type of _value\n\n    union CarName {\n        char charName,  // use a character as a name\n        int intName,    // use a number as a name\n        char *strName,  // use a string as a name\n    } _value;\n\n} value;\n</code></pre> <p>To use tagged union: <pre><code>if (value.tag == 0) {\n    printf(\"Car name is a character: %c\", value._value.charname);\n}\nelse if (value.tag ==  ...) {\n    ...\n}\n</code></pre></p> <p>It alows writing a user-defined type that: * Has a tag to indicate which data type it belong to. * values of same tagged-union type have same size (the biggest)</p> <p>Advantage:  * have cheking -&gt; more safety.</p> <p>Disadvantage: * still have to remember the data type. * the checking might have to cover alot of cases.</p>"},{"location":"rust/enums/enums/#2-enum-in-rust","title":"2. Enum in Rust","text":"<p>In Rust, enum is implemented as tagged-union.</p> <p><pre><code>enum MyEnum {\n    ENumber(i32),\n    EString(String),\n    ETupple(i32, i32, i32, i32),\n}\n</code></pre> </p> <p>Size of an enum is the size of tag pluse size of biggest member.</p> <p>Ref: Enum section</p>"},{"location":"rust/introduce/introduce/","title":"Introduce","text":""},{"location":"rust/introduce/introduce/#brieft-introduction-to-rust","title":"Brieft introduction to Rust","text":"<p>ThongDinh -ft- DatDinh</p>"},{"location":"rust/introduce/introduce/#contents","title":"Contents","text":"<ul> <li>Get to know Rust</li> <li>History of Rust</li> <li>Notable features</li> <li>Get dive deeper to know Rust</li> <li>How Rust archives memory safety</li> </ul>"},{"location":"rust/introduce/introduce/#a-get-to-know-rust","title":"A. Get to know Rust","text":"<ul> <li>History of Rust</li> <li>Notable features</li> </ul>"},{"location":"rust/introduce/introduce/#history-of-rust","title":"History of Rust","text":"<ul> <li>When: 2006</li> <li>Who: Graydon Hoare, engineer at Mozilla Research</li> <li>What: <ul> <li>Offered high performance of C/C++ </li> <li>With guaranteed memory safety</li> </ul> </li> <li>Which:<ul> <li>linux kernel, OS, OS-utilities</li> <li>block chain, wasm</li> <li>backend</li> <li>... </li> </ul> </li> </ul>"},{"location":"rust/introduce/introduce/#rust-inside","title":"Rust inside","text":""},{"location":"rust/introduce/introduce/#sponsors","title":"Sponsors","text":""},{"location":"rust/introduce/introduce/#notable-features","title":"Notable Features","text":""},{"location":"rust/introduce/introduce/#memory-safety-rust-vs-cc","title":"Memory Safety. Rust vs C/C++","text":"<ul> <li>C/C++ easily to have error:</li> <li> <p>dangling pointers:</p> <ul> <li>use-after-free</li> <li>double-free</li> <li>buffer overflows</li> </ul> </li> <li> <p>Rust Compiler:</p> </li> <li>Throws errors on compilation if it detect codes violate it's rules. </li> <li>Virtually guaranteed to be memory-safe if compile successfully</li> </ul>"},{"location":"rust/introduce/introduce/#memory-safety-rust-vs-javago","title":"Memory Safety. Rust vs Java/Go","text":"<ul> <li>Java/Go use GC:</li> <li>Automatically finds and removes unused objects in the heap.</li> <li>Pros:<ul> <li>Ease of Development + Platform Independence</li> </ul> </li> <li> <p>Cons:</p> <ul> <li>CPU Overhead + Unpredictable Pauses + extra memory for GC (bloat)</li> </ul> </li> <li> <p>Rust use NO GC:</p> </li> <li>no runtime overhead</li> <li>no extra memory</li> </ul>"},{"location":"rust/introduce/introduce/#performance","title":"Performance","text":"<ul> <li>Build to native machine code --&gt; fast ~ C/C++</li> <li>No GC -&gt; faster than Java, Go</li> <li>Zero-cost abstraction (same as C++ if not using vtable)</li> <li>Fearless concurency by the advantage of Ownership model</li> </ul>"},{"location":"rust/introduce/introduce/#performance_1","title":"Performance","text":"<p>source</p>"},{"location":"rust/introduce/introduce/#performance-benchmark-vs-c","title":"Performance: benchmark vs C","text":"<p>https://programming-language-benchmarks.vercel.app/c-vs-rust</p>"},{"location":"rust/introduce/introduce/#performance-benchmark-vs-go","title":"Performance: benchmark vs GO","text":"<p>source</p>"},{"location":"rust/introduce/introduce/#concurrency","title":"Concurrency","text":"<ul> <li>\"Fearless Concurrency\":</li> <li>No data race by the advantage of Ownership model</li> <li>Offers modern concurrency paradigms:</li> <li>Message passing via Channel</li> <li>async/await</li> </ul>"},{"location":"rust/introduce/introduce/#speed-concurrent-programming","title":"Speed: concurrent programming","text":"<ul> <li>source: trust me bro</li> </ul>"},{"location":"rust/introduce/introduce/#take-away-1","title":"Take away 1:","text":"<ul> <li>Pros:<ul> <li>Guarantee to be memory safe</li> <li>Offer high speed approximate to C/C++</li> </ul> </li> <li>Cons:<ul> <li>Hard to learn at the beginning</li> <li>Compilation cost a lot of time due to checking alot of rules</li> </ul> </li> </ul>"},{"location":"rust/introduce/introduce/#b-dive-deeper","title":"B. Dive deeper","text":"<ul> <li>How Rust archives memory safety</li> </ul>"},{"location":"rust/introduce/introduce/#memory-unsafety-in-cc","title":"Memory unsafety in C/C++","text":""},{"location":"rust/introduce/introduce/#how-rust-archives-memory-safety","title":"How Rust archives memory safety","text":""},{"location":"rust/introduce/introduce/#how-rust-archives-memory-safety_1","title":"How Rust archives memory safety","text":""},{"location":"rust/introduce/introduce/#ownership-model","title":"Ownership model","text":""},{"location":"rust/introduce/introduce/#ownership-model_1","title":"Ownership model","text":""},{"location":"rust/introduce/introduce/#lifetime","title":"Lifetime","text":""},{"location":"rust/introduce/introduce/#multithread","title":"Multithread","text":""},{"location":"rust/introduce/introduce/#multithread-sharing-ownership","title":"Multithread. Sharing Ownership","text":""},{"location":"rust/introduce/introduce/#multithread-sharing-reference","title":"Multithread. Sharing Reference","text":""},{"location":"rust/introduce/introduce/#q-a","title":"Q &amp; A","text":"<p>Thank you !!!</p>"},{"location":"rust/lifetime/lifetime/","title":"Lifetimes","text":"<p>Lifetimes of a reference is about: * What regions of memory a reference could points to ? * When the regions are valid?</p>"},{"location":"rust/lifetime/lifetime/#what-regions-of-memory","title":"What regions of memory ?","text":"<pre><code>fn main() {\n    let r;\n    {\n        let x = 5;\n        // before\n        r = &amp;x;\n    }\n    // after\n    println!(\"r: {r}\");     // compile error here\n}\n</code></pre>"},{"location":"rust/lifetime/lifetime/#lifetime-anotation","title":"Lifetime anotation","text":""},{"location":"rust/lifetime/lifetime/#why-explicit-lifetime-needed","title":"Why explicit lifetime needed","text":"<pre><code>fn longest(x: &amp;str, y: &amp;str) -&gt; &amp;str {\n    x\n}\n...\nlet output : &amp;str = longest();\n</code></pre> <pre><code>fn foo&lt;'a&gt;(x: &amp;'a u32, y: &amp;'a u32) -&gt; &amp;'a u32 {\n    if (x &gt; y) {\n        x\n    }\n    else {\n        y\n    }\n}\n</code></pre> <pre><code>fn main() {\n    let x = 12;\n    let z: &amp;u32 = {\n        let y = 42;\n        foo(&amp;x, &amp;y)\n    };\n\n    println!(\"hello world {z}\");\n}\n</code></pre>"},{"location":"rust/memory_layout/memory_layout/","title":"Memory layout of some data types","text":""},{"location":"rust/memory_layout/memory_layout/#struct-tupple","title":"Struct &amp; tupple","text":"<pre><code>struct User {\n    id: char,\n    active: u8,\n    age: i32,\n    ...\n}\n</code></pre> <pre><code>// tupple lives on stack\nlet tupple : (char, u8 i32) = ('a', 100, 500);\n</code></pre>"},{"location":"rust/memory_layout/memory_layout/#array","title":"Array","text":"<p><pre><code>// array lives on Stack\nlet array : [i32, 3] = [5, 10, 15];\n</code></pre> </p>"},{"location":"rust/memory_layout/memory_layout/#reference","title":"Reference","text":"<ul> <li>Reference contains an address.</li> <li> <p>8bytes on 64bit OS. 4 bytes on 32bit OS.</p> </li> <li> <p>Mutable &amp; Immutable reference are same in memory layout. It's a deference in how they're used.</p> </li> </ul> <pre><code>let i : i32 = 10;\nlet ref : &amp;i32 = &amp;i;\n</code></pre> <p></p>"},{"location":"rust/memory_layout/memory_layout/#vector","title":"Vector","text":"<p><pre><code>let v: Vec&lt;i32&gt; = vec![5, 10, 15];\nlet sl : &amp;[i32] = &amp;v[0..2];\n</code></pre> </p>"},{"location":"rust/memory_layout/memory_layout/#string","title":"String","text":"<p>String * String is standard container.</p> <p><pre><code>let hello_String = String::from(\"Hello\")\n</code></pre> </p> <p>Literal string * str is rust's core string type. * Literal string points to data in Code segment <pre><code>let literal_str = \"Literal String\"\nlet slice = &amp;literal_str[8..]\n// str and slice have same memory layout\n</code></pre> </p>"},{"location":"rust/memory_layout/memory_layout/#enum","title":"Enum","text":"<p><pre><code>// C style enum\nenum IP{\n    v4,\n    v6 = 500,\n}\n\n// C style union\nunion MyTag {\n    int i;\n    double d;\n    char s[16];\n}\n\n// Rust style enum: tagged union\nenum CarTag{\n    TagNumber(i32),\n    TagString(String),\n}\n</code></pre> </p> <p></p> <ul> <li> <p>Size of enum IP is the size of the highest value.</p> </li> <li> <p>Size of enum CarTag is the sum of sizes of tag, TagString</p> </li> <li> <p>Using smartpoint, e.g: Box, to reduce the size of enum</p> </li> </ul> <p><pre><code>enum CarTag{\n    ...\n    TagBox(Box&lt;String&gt;),    // Box is pointer which is fixed size\n    ...\n}\n</code></pre> </p> <ul> <li> <p>Rust automatically optimizes <code>Options&lt;T&gt;</code> which contains smart pointer data type (e.g: Box) <pre><code>// Option\nenum Option&lt;T&gt;{\n    None\n    Some(T),\n}\n\n// Option with smart pointer\nlet opt: Options&lt;Box&lt;i32&gt;&gt; = Options(Box::new(100)));\n</code></pre> </p> </li> <li> <p>In case of optimization Options, the matching pattern works by check if the pointer is NULL or not, corresponding to None and Some <li> <p>Matching pattern with enum is just a comparation of the tag value.</p> </li> <p></p>"},{"location":"rust/memory_layout/memory_layout/#box","title":"Box","text":"<ul> <li>Use smart pointer Box&lt;&gt; to store data on Heap</li> </ul> <pre><code>    let b = Box::new(5);\n</code></pre> <ul> <li> <p>There a 2 use case:</p> <ul> <li> <p>Case 1: Declare a variable which has unknow size at compile time <pre><code>trait Vehicle { \n    fn drive(&amp;self);\n}\n\nlet t : dyn Vehicle         // Wrong: declare t as an unknow size data\nlet t : Box&lt;dyn Vehicle&gt;;   // Correct: declare t as a pointer which point to trai object in heap\nt = Box::new(Car);\nt.drive();\n</code></pre></p> </li> <li> <p>Case 2: Recursive data type. <pre><code>// : \nenum List {\n    Cons(i32, List),        // Wrong: size of List is unknow\n    Cons(i32, Box&lt;List&gt;),   // Correct: sizeof Box is known\n    Nil,\n}\n</code></pre> </p> </li> </ul> </li> </ul>"},{"location":"rust/memory_layout/memory_layout/#copy-clone-trait","title":"Copy &amp; Clone Trait","text":"<pre><code>// default: move semantics\nlet s1 = String::from(\"Hello\");\nlet s2 = s1;            // s1 no longer valid\n\n// Implicit in-expensive bit-wise copy\nlet a1: i32 = 10;\nlet a2: i32 = a1;\n\n// Explicit bit-wise copy\nlet s2 = s1.clone();    // s1 and s2 both are valid\n</code></pre> <ul> <li>Clone is supertrait of Copy</li> <li>Copy is simply a bit-wise copy</li> </ul>"},{"location":"rust/memory_layout/memory_layout/#trait-object","title":"Trait Object","text":"<ul> <li>Reference or pointer of value, which implement a Trait, is called Trait Object: <pre><code>trait Shape {\n    fn area(&amp;self);\n}\n\nstruct Rectangle {\n    top_left: ...,\n    bottom_right: ...,\n}\n\nimpl Shape for Rectangle {\n    fn area(...)\n}\n\nlet rec = Rectangle;\n\nlet t : dyn Shape;  // Wrong: declare t as an unknow size data\nlet t : &amp;dyn Shape = &amp;rec;      // Correct\nlet t : Box&lt;dyn Shape&gt; = &amp;rec;  // Correct\n// trait object is known size.\n// trait object is a fat pointer which contains 2 other pointers\n</code></pre></li> </ul> <p>Use trait object for Dynamic dispatching. Read more on Trait bound and dispatching ...</p>"},{"location":"rust/memory_layout/memory_layout/#closure","title":"Closure","text":"<p>Rust uses struct to present closure.</p> <pre><code>let color = String::from(\"green\");\nlet print = || println!(\"color: {}\", color);  \n// print is a closure\n\n// exec closure:\nprint()\n</code></pre> <p></p> <p>Read more on closure ...</p>"},{"location":"rust/memory_layout/memory_layout/#reference-count","title":"Reference count","text":"<ul> <li>Using RC to have multiple pointers point to the same value</li> </ul> <ul> <li>Data race in multithread:</li> </ul> <ul> <li> <p>Use Atomic Reference Count (Arc) to avoid data race.</p> <p>Arc costs small extra performance.</p> <p>Both Rc &amp; Arc are immutable. Use mutex to mutate the data. <pre><code>let arc_mutext : Arc&lt;Mutex&lt;i32&gt;&gt; = Arc::new(Mutex::new(100))\n</code></pre></p> </li> </ul> <p></p>"},{"location":"rust/send_sync/send_sync/","title":"Send &amp; Sync Trait","text":"<p>Send and Sync are too traits which used to mark a data type.</p>"},{"location":"rust/send_sync/send_sync/#what-is-sync","title":"What is Sync","text":"<p>A Data type marked as Sync means: * Multiple threads can use it at same time.</p> <p></p>"},{"location":"rust/send_sync/send_sync/#what-is-send","title":"What is Send","text":"<p>A Data type marked as Send means: * data can be moved from a thread to another thread. * Or, multiple threads have chance to hold it. But at a point of time, only one thread uses it.</p> <p></p> <p>Ofcouse Sync is Send. But Send is not Sync.</p>"},{"location":"rust/trai_bound_and_dispatching/trai_bound_and_dispatching/","title":"Trait bound and ways for dispatching","text":""},{"location":"rust/trai_bound_and_dispatching/trai_bound_and_dispatching/#trait-bound","title":"Trait bound","text":"<ul> <li>Trait is not a type.</li> <li>Trait is a bound on type.</li> </ul> <pre><code>struct Bar;\ntrait Foo {\n    fn foo();\n}\nimpl Foo for Bar {...}\n</code></pre> <p>Dispatching is how we pass a value, which implement trait, to a function?     * At runtime, do we know the data type of passed value or not?     * The value is known size, or un-known size?</p> <p>Normally, we might write a prototype like bellow, but it not correct in Rust. <pre><code>// Compiler doesn't know which struct impl Foo will be passed in\n// -&gt; doesn't know size of params\nfn my_func(foo: Foo) {      // Wrong: Foo is a trait, not a type\n    ...\n}\n</code></pre></p> <p>There are 2 ways in contrast for dispatching:     * Static dispatching     * Dynamic dispatching</p>"},{"location":"rust/trai_bound_and_dispatching/trai_bound_and_dispatching/#static-dispatching","title":"Static dispatching","text":""},{"location":"rust/trai_bound_and_dispatching/trai_bound_and_dispatching/#using-generic-data-types","title":"Using generic data types","text":"<pre><code>// pass by value\nfn my_func&lt;T : Foo&gt; (t : T) {}\n// pass by reference\nfn my_func&lt;T : Foo&gt; (t : &amp;T) {}\n// pass by Boxed reference\nfn my_func&lt;T : Foo&gt; (t : Box&lt;T&gt;) {}\n</code></pre> <ul> <li>When to use:<ul> <li>when have two params, and both have same Data type. Because the data type must be determined at compile time.</li> </ul> </li> </ul>"},{"location":"rust/trai_bound_and_dispatching/trai_bound_and_dispatching/#using-imp-trait","title":"Using imp Trait","text":"<p>This way is the same as using generic data types.</p> <pre><code>// pass by value\nfn my_func(t: impl Foo) {}\n// pass by reference\nfn my_func(t: &amp;impl Foo) {}\n// pass by boxed reference\nfn my_func(t: Box&lt;impl Foo&gt;) {}\n</code></pre> <ul> <li>When to use:<ul> <li>exactly same as Using generic data types</li> </ul> </li> </ul>"},{"location":"rust/trai_bound_and_dispatching/trai_bound_and_dispatching/#pros-cons","title":"Pros &amp; Cons","text":"<p>Pros:     * The data type is known at compile time.     * Member function of data type will be call directly</p> <p>Cons:     * Rust uses monomorphization to perform Generic static dispatching.     It means, in case of using generic data types, if we use my_func() for several concrete types, rust will create several version of my_func() corresponding to each data type.</p> <pre><code>struct Bar;\nimpl Foo for Bar {...}\n\nstruct Bazzz;\nimpl Foo for Bazzz {...}\n\nlet bar Bar;\nlet bazzz = Bazzz;\n\nmy_func(bar)\n// my_func_bar(t: Bar), a version of my_func for Bar, will be used instead of my_func\n\nmy_func(bazzz)\n// my_func_bazzz(t: Bazzz), a version of my_func for Bazzz, will be used instead of my_func\n</code></pre>"},{"location":"rust/trai_bound_and_dispatching/trai_bound_and_dispatching/#dynamic-dispatching","title":"Dynamic dispatching","text":"<p>Passing by Trait object. Trait object is a refrence to instance of data type implemented the trait.</p> <p>Trait object actually is a fat-pointer, which contains 2 child pointers:     * Pointer to data     * Pointer to vTable, which indirectly point to member function of data type.</p> <pre><code>pub struct TraitObject {\n    pub data: *mut (),      // raw pointer to void (unsafe)\n    pub vtable: *mut (),\n</code></pre> <p>Ref: Trait Object</p> <p></p> <pre><code>fn my_func(t : &amp;dyn Foo)    // dyn ~ dynamic dispatch\nfn my_func(t : &amp;Foo)        // coercing\nfn my_func(t : Box&lt;Foo&gt;)    // smart pointer\n</code></pre>"},{"location":"rust/trai_bound_and_dispatching/trai_bound_and_dispatching/#pros-cons_1","title":"Pros &amp; Cons","text":"<p>Pros:     * Can bring up Polymorphism feature.</p> <p>Cons:     * Member function will be call in-directly (2 times de-reference)</p>"},{"location":"rust/trai_bound_and_dispatching/trai_bound_and_dispatching/#example","title":"Example","text":"<pre><code>struct Honda {\n    name: String,\n    size: i32,\n    weight: i32,\n    ...\n}\n\ntrait VehicleAction {\n    fn drive()(&amp;self);\n}\n\nimpl VehicleAction for Honda {\n    fn drive(&amp;self) {...};\n}\n\n// Static dispatch\nfn drive_vehicle_static(vehicle: &amp;impl VehicleAction) {\n    vehicle.drive();\n}\n\n// dynamic dispatch\nfn drive_vehicle_dynamic(vehicle: &amp;dyn VehicleAction) {\n    vehicle.drive();\n}\n</code></pre>"}]}